---
layout: default
title: CSE 332
parent: CSE
---

# CSE 332

---

## Lecture 1 - June 21

- What this course is:
    - Classic data structurs and algos
    - Queues, dicts, graphs, sorting
    - Parallelism and concurrency (new! exciting!)
- Data structures are clever ways of storing information to perform efficent compututation
    - There are *always* tradeoffs with each option
- Abstract Data Type (ADT) is a mathematical description of a "thing" with a set of operations
- Data Structure: specific orginization of data and family of algos for implementing ADT
- Implementation: the actual concrete code
- We can use circular arrays or double-linked lists for queues
    - Both work!

---

## Lecture 2 - June 23

- What do we care about? (DSA)
    1. Correctness - does it work?
    2. Performance - speed! (and memory)
- Evaluating the *program* is difficult - better to evaluate the *algorithm*
- Analyzing code: counting code constructs:
    - Basic operations (indexing, assignment): $\text{constant time}$
    - Loops: $\text{num iterations} \times \text{loop body}$
    - Conditionals: $\text{time of condition} + \text{slowest branch}$
    - Function calls: $\text{timee of functions body}$
    - Recursion: $\text{solve recurrence equation}$
- Complexity cases:
    - Worst-case complexity: max num of steps algo takes on "most challenging" input $n$
    - Best-case complexity: min num of steps algo takes on "easiest" input of size $n$
    - Average-case complexity: *what does "average" mean?*
    - Amortized-case complexity: we learn about this later
- Linear search, best case: $O(1)$, worst case: $O(n)$
- Asymptotic analysis: big $O$ notation
    - A function $f$ is in $O(\tilde{f})$ if $f$ and $\tilde{f}$ have the same asymptoic behavior

---

## Lecture 3 - June 26

- How to show $f(n)$ is in $O(g(n))$?
    - Pick a $c$ large enough to cover the constant factors
    - Pick a $n_0$ large enough to cover the lower order terms
- Dropping coefficents is ususally okay, be careful otherwise
- Upper bound: Big-O
- Lower bound: Big-Omega, same as upper bound but for min
- Big-Theta bound (tight bound): $f(n)$ is in the upper and lower bounds of $g(n)$
- Worst/best-case vs asymptotic analysis
    - Think of it as comparing scenarios
- Amortization: worst-case is too pessimistic
    - "what is the average run time of operations in the *worst* sequence?"
    - **Not** the average (average is an average input, this is average operations over a sequence of worst-case operations)

---

## Lecture 4 - June 28

- Priority Queue: Highest Priority, First Out
    - Holds comparable data
    - `insert`: adds an item at the end
    - `deleteMin`: finds, returns, and removes the minimum element in the queue
- Heap: only pay for functionality needed (half-sorted)
    - Similar to trees!
    - Both operations are: $\Theta(log n)$
- Tree terminology:
    - `depth`: how many levels from root to node
    - `height`: how far from "deepest" descendent leaf node
- Binary tree: each node has max 2 children
- `n`-ary tree: each node has max `n` children
- Perfect tree: each row is completely full
- Complete tree: each row is completely full except the bottom row
    - The bottom row must be filled up from left to right
- (Binary Min-) Heap:
    - Complete binary tree structure
    - Every (non-root) node has a priority value greater than its parent
    - `insert:`
        - Preserve complete tree structure property
        - This might break heap order property
        - Percolate up to restore heap order property
    - `deleteMin:`
        - Remove root node
        - Move bottom side node to root
        - This might break heap order property
        - Percolate down to restore property
- We can use an array to store the heap
    - We will skip index 0 to make the math easier
    - To get to the child nodes from node `i`, go to `i*2` and `i*2 + 1`
    - Parent is integer division
- To increase or decrease a key by an amount, we change it then percolate
- To delete, we simply decrease the key by infinity, then delete min

---

## Lecture 5 - June 30

- Naively building a heap: `insert` for each element
    - This is $O(n * log n)$
- We can do better: Floyd's `buildHeap`
    - $O(n)$ runtime
    - Randomly add all elements into the array
    - Percolate down from each element one level above the leaves, up to the root
    - As each level increases the amount of operations but halves the number of iterations, it is $O(n)$
- Counting recursive code:
    - Each recursive method call:
        - Perform some *non-recursive* work: $w(n)$
        - Call the method $T(n)$ on a smaller part of the list $T(n-1)$ (where $T(n)$ is the time or cost function)
        - Do some base case work: $b(n)$
- Total cost: $T(n) = w(n) + T(n - 1)$ where some base case: $T(1) = b(n)$
    - This is basically the recurrence function/relation
    - General formula and closed form are ways to "solve" the function
- How to solve a recurrence function?
- Unrolling: substitution until we find a pattern
    1. Write a recurrence
    2. Find general formula: expand
        - Should still have a $T$ function on both sides
    3. Find closed form: find when base case occurs, then get to the base case
        - Now no $T$ or $i$: all constants or $n$
    4. Asymptotic analysis

---

## Lecture 6 - June 3

- Recurrence relation: Tree Method
    - We draw an actual tree of the work required for function calls
    - Allows us to find the amount of nodes quickly and the work per node
- Common recurrences:
    - $T(n) = T(\frac{n}{2}) + c \in \Theta (log n)$: Binary search
    - $T(n) = 2T(\frac{n}{2}) + n \in \Theta (n log n)$: Merge sort
    - $T(n) = 2T(\frac{n}{2}) + c \in \Theta (n)$: Recursive binary sum
    - $T(n) = T(n - 1) + c \in \Theta (n)$: Recursive sum
- Dictionary: associate associates `key`, `value` pairs
    - Set of unique `key`, `value` pairs
    - (For some) keys must be comparable
    - Operations:
        - `insert(k, v)`: place `k`, `v` in dictionary
        - `find(k)`: return the `v` associated with `k`
        - `delete(k)`: delete and return the `k`, `v`
- Set is just a dictionary with boolean values
- Naieve implementation operations are $\Theta (n)$
    - `find(k)` is $\Theta (log n)$ for sorted arrays

---

## Lecture 7 - June 5

- Many data structure operations come down to preserving structure and order properties
- Comparable keys allow for ordering in storage
- Lazy deletion: we only "mark" as deleted - is the element there?
    - Allows for removing in batches
    - Increases `find(k)` runtime
- Binary search trees (BST) have "average" $\Theta (log n)$ runtime, but could be $\Theta (n)$
    - All elements are either roots, left subtrees, or right subtrees
- BST Delete:
    - Case 1: Leaf
        - Just remove the leaf
    - Case 2: One child
        - Connect parent to the child node
    - Case 3: Two children
        - We can replace with smallest element from right tree or largest from left tree, predecessor or successor
        - Find the max on the left subtree, swap with parent then delete
        - Find the min on the right subtree, swap with parent then delete
- Worse case for BST occurs when the tree is unbalanced
- Ideas to balance the BST: (while keeping fast runtime)
    - Left and right subtree of `root` have same number of nodes
        - Too weak
    - Left and right subtree of `root` have same height
        - Too weak
    - Left and right subtree of every tree have same number of nodes
        - Perfect tree, too strong
    - Left and right subtree of every tree have same height
        - Perfect tree, too strong
    - Left and right subtree of each node's heights differ by at most 1
        - Ensures `root` height is $\Theta (log n)$
        - Quick: $\Theta (1)$ rotations
- AVL Balance Property: `balance(node) = height(node.left) - height(node.right)`
- AVL Tree: a BST with guaranteed balancing
    - BST with the added condition that for all `node`s, `-1 <= balance(node) <= 1`
    - We need to keep track of the heights
