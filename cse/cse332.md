---
layout: default
title: CSE 332
parent: CSE
---

# CSE 332

---

## Lecture 1 - June 21

- What this course is:
    - Classic data structurs and algos
    - Queues, dicts, graphs, sorting
    - Parallelism and concurrency (new! exciting!)
- Data structures are clever ways of storing information to perform efficent compututation
    - There are *always* tradeoffs with each option
- Abstract Data Type (ADT) is a mathematical description of a "thing" with a set of operations
- Data Structure: specific orginization of data and family of algos for implementing ADT
- Implementation: the actual concrete code
- We can use circular arrays or double-linked lists for queues
    - Both work!

---

## Lecture 2 - June 23

- What do we care about? (DSA)
    1. Correctness - does it work?
    2. Performance - speed! (and memory)
- Evaluating the *program* is difficult - better to evaluate the *algorithm*
- Analyzing code: counting code constructs:
    - Basic operations (indexing, assignment): $\text{constant time}$
    - Loops: $\text{num iterations} \times \text{loop body}$
    - Conditionals: $\text{time of condition} + \text{slowest branch}$
    - Function calls: $\text{timee of functions body}$
    - Recursion: $\text{solve recurrence equation}$
- Complexity cases:
    - Worst-case complexity: max num of steps algo takes on "most challenging" input $n$
    - Best-case complexity: min num of steps algo takes on "easiest" input of size $n$
    - Average-case complexity: *what does "average" mean?*
    - Amortized-case complexity: we learn about this later
- Linear search, best case: $O(1)$, worst case: $O(n)$
- Asymptotic analysis: big $O$ notation
    - A function $f$ is in $O(\tilde{f})$ if $f$ and $\tilde{f}$ have the same asymptoic behavior

---

## Lecture 3 - June 26

- How to show $f(n)$ is in $O(g(n))$?
    - Pick a $c$ large enough to cover the constant factors
    - Pick a $n_0$ large enough to cover the lower order terms
- Dropping coefficents is ususally okay, be careful otherwise
- Upper bound: Big-O
- Lower bound: Big-Omega, same as upper bound but for min
- Big-Theta bound (tight bound): $f(n)$ is in the upper and lower bounds of $g(n)$
- Worst/best-case vs asymptotic analysis
    - Think of it as comparing scenarios
- Amortization: worst-case is too pessimistic
    - "what is the average run time of operations in the *worst* sequence?"
    - **Not** the average (average is an average input, this is average operations over a sequence of worst-case operations)

---

## Lecture 4 - June 28

- Priority Queue: Highest Priority, First Out
    - Holds comparable data
    - `insert`: adds an item at the end
    - `deleteMin`: finds, returns, and removes the minimum element in the queue
- Heap: only pay for functionality needed (half-sorted)
    - Similar to trees!
    - Both operations are: $\Theta(log n)$
- Tree terminology:
    - `depth`: how many levels from root to node
    - `height`: how far from "deepest" descendent leaf node
- Binary tree: each node has max 2 children
- `n`-ary tree: each node has max `n` children
- Perfect tree: each row is completely full
- Complete tree: each row is completely full except the bottom row
    - The bottom row must be filled up from left to right
- (Binary Min-) Heap:
    - Complete binary tree structure
    - Every (non-root) node has a priority value greater than its parent
    - `insert:`
        - Preserve complete tree structure property
        - This might break heap order property
        - Percolate up to restore heap order property
    - `deleteMin:`
        - Remove root node
        - Move bottom side node to root
        - This might break heap order property
        - Percolate down to restore property
- We can use an array to store the heap
    - We will skip index 0 to make the math easier
    - To get to the child nodes from node `i`, go to `i*2` and `i*2 + 1`
    - Parent is integer division
- To increase or decrease a key by an amount, we change it then percolate
- To delete, we simply decrease the key by infinity, then delete min

---

## Lecture 5 - June 30

- Naively building a heap: `insert` for each element
    - This is $O(n * log n)$
- We can do better: Floyd's `buildHeap`
    - $O(n)$ runtime
    - Randomly add all elements into the array
    - Percolate down from each element one level above the leaves, up to the root
    - As each level increases the amount of operations but halves the number of iterations, it is $O(n)$
- Counting recursive code:
    - Each recursive method call:
        - Perform some *non-recursive* work: $w(n)$
        - Call the method $T(n)$ on a smaller part of the list $T(n-1)$ (where $T(n)$ is the time or cost function)
        - Do some base case work: $b(n)$
- Total cost: $T(n) = w(n) + T(n - 1)$ where some base case: $T(1) = b(n)$
    - This is basically the recurrence function/relation
    - General formula and closed form are ways to "solve" the function
- How to solve a recurrence function?
- Unrolling: substitution until we find a pattern
    1. Write a recurrence
    2. Find general formula: expand
        - Should still have a $T$ function on both sides
    3. Find closed form: find when base case occurs, then get to the base case
        - Now no $T$ or $i$: all constants or $n$
    4. Asymptotic analysis

