---
layout: default
title: CSE 332
parent: CSE
---

# CSE 332

---

## Lecture 1 - June 21

- What this course is:
    - Classic data structurs and algos
    - Queues, dicts, graphs, sorting
    - Parallelism and concurrency (new! exciting!)
- Data structures are clever ways of storing information to perform efficent compututation
    - There are *always* tradeoffs with each option
- Abstract Data Type (ADT) is a mathematical description of a "thing" with a set of operations
- Data Structure: specific orginization of data and family of algos for implementing ADT
- Implementation: the actual concrete code
- We can use circular arrays or double-linked lists for queues
    - Both work!

---

## Lecture 2 - June 23

- What do we care about? (DSA)
    1. Correctness - does it work?
    2. Performance - speed! (and memory)
- Evaluating the *program* is difficult - better to evaluate the *algorithm*
- Analyzing code: counting code constructs:
    - Basic operations (indexing, assignment): $\text{constant time}$
    - Loops: $\text{num iterations} \times \text{loop body}$
    - Conditionals: $\text{time of condition} + \text{slowest branch}$
    - Function calls: $\text{timee of functions body}$
    - Recursion: $\text{solve recurrence equation}$
- Complexity cases:
    - Worst-case complexity: max num of steps algo takes on "most challenging" input $n$
    - Best-case complexity: min num of steps algo takes on "easiest" input of size $n$
    - Average-case complexity: *what does "average" mean?*
    - Amortized-case complexity: we learn about this later
- Linear search, best case: $O(1)$, worst case: $O(n)$
- Asymptotic analysis: big $O$ notation
    - A function $f$ is in $O(\tilde{f})$ if $f$ and $\tilde{f}$ have the same asymptoic behavior

---

## Lecture 3 - June 26

- How to show $f(n)$ is in $O(g(n))$?
    - Pick a $c$ large enough to cover the constant factors
    - Pick a $n_0$ large enough to cover the lower order terms
- Dropping coefficents is ususally okay, be careful otherwise
- Upper bound: Big-O
- Lower bound: Big-Omega, same as upper bound but for min
- Big-Theta bound (tight bound): $f(n)$ is in the upper and lower bounds of $g(n)$
- Worst/best-case vs asymptotic analysis
    - Think of it as comparing scenarios
- Amortization: worst-case is too pessimistic
    - "what is the average run time of operations in the *worst* sequence?"
    - **Not** the average (average is an average input, this is average operations over a sequence of worst-case operations)

---

## Lecture 4 - June 28

- Priority Queue: Highest Priority, First Out
    - Holds comparable data
    - `insert`: adds an item at the end
    - `deleteMin`: finds, returns, and removes the minimum element in the queue
- Heap: only pay for functionality needed (half-sorted)
    - Similar to trees!
    - Both operations are: $\Theta(log n)$
- Tree terminology:
    - `depth`: how many levels from root to node
    - `height`: how far from "deepest" descendent leaf node
- Binary tree: each node has max 2 children
- `n`-ary tree: each node has max `n` children
- Perfect tree: each row is completely full
- Complete tree: each row is completely full except the bottom row
    - The bottom row must be filled up from left to right
- (Binary Min-) Heap:
    - Complete binary tree structure
    - Every (non-root) node has a priority value greater than its parent
    - `insert:`
        - Preserve complete tree structure property
        - This might break heap order property
        - Percolate up to restore heap order property
    - `deleteMin:`
        - Remove root node
        - Move bottom side node to root
        - This might break heap order property
        - Percolate down to restore property
- We can use an array to store the heap
    - We will skip index 0 to make the math easier
    - To get to the child nodes from node `i`, go to `i*2` and `i*2 + 1`
    - Parent is integer division
- To increase or decrease a key by an amount, we change it then percolate
- To delete, we simply decrease the key by infinity, then delete min

---

## Lecture 5 - June 30

- Naively building a heap: `insert` for each element
    - This is $O(n * log n)$
- We can do better: Floyd's `buildHeap`
    - $O(n)$ runtime
    - Randomly add all elements into the array
    - Percolate down from each element one level above the leaves, up to the root
    - As each level increases the amount of operations but halves the number of iterations, it is $O(n)$
- Counting recursive code:
    - Each recursive method call:
        - Perform some *non-recursive* work: $w(n)$
        - Call the method $T(n)$ on a smaller part of the list $T(n-1)$ (where $T(n)$ is the time or cost function)
        - Do some base case work: $b(n)$
- Total cost: $T(n) = w(n) + T(n - 1)$ where some base case: $T(1) = b(n)$
    - This is basically the recurrence function/relation
    - General formula and closed form are ways to "solve" the function
- How to solve a recurrence function?
- Unrolling: substitution until we find a pattern
    1. Write a recurrence
    2. Find general formula: expand
        - Should still have a $T$ function on both sides
    3. Find closed form: find when base case occurs, then get to the base case
        - Now no $T$ or $i$: all constants or $n$
    4. Asymptotic analysis

---

## Lecture 6 - June 3

- Recurrence relation: Tree Method
    - We draw an actual tree of the work required for function calls
    - Allows us to find the amount of nodes quickly and the work per node
- Common recurrences:
    - $T(n) = T(\frac{n}{2}) + c \in \Theta (log n)$: Binary search
    - $T(n) = 2T(\frac{n}{2}) + n \in \Theta (n log n)$: Merge sort
    - $T(n) = 2T(\frac{n}{2}) + c \in \Theta (n)$: Recursive binary sum
    - $T(n) = T(n - 1) + c \in \Theta (n)$: Recursive sum
- Dictionary: associate associates `key`, `value` pairs
    - Set of unique `key`, `value` pairs
    - (For some) keys must be comparable
    - Operations:
        - `insert(k, v)`: place `k`, `v` in dictionary
        - `find(k)`: return the `v` associated with `k`
        - `delete(k)`: delete and return the `k`, `v`
- Set is just a dictionary with boolean values
- Naieve implementation operations are $\Theta (n)$
    - `find(k)` is $\Theta (log n)$ for sorted arrays

---

## Lecture 7 - June 5

- Many data structure operations come down to preserving structure and order properties
- Comparable keys allow for ordering in storage
- Lazy deletion: we only "mark" as deleted - is the element there?
    - Allows for removing in batches
    - Increases `find(k)` runtime
- Binary search trees (BST) have "average" $\Theta (log n)$ runtime, but could be $\Theta (n)$
    - All elements are either roots, left subtrees, or right subtrees
- BST Delete:
    - Case 1: Leaf
        - Just remove the leaf
    - Case 2: One child
        - Connect parent to the child node
    - Case 3: Two children
        - We can replace with smallest element from right tree or largest from left tree, predecessor or successor
        - Find the max on the left subtree, swap with parent then delete
        - Find the min on the right subtree, swap with parent then delete
- Worse case for BST occurs when the tree is unbalanced
- Ideas to balance the BST: (while keeping fast runtime)
    - Left and right subtree of `root` have same number of nodes
        - Too weak
    - Left and right subtree of `root` have same height
        - Too weak
    - Left and right subtree of every tree have same number of nodes
        - Perfect tree, too strong
    - Left and right subtree of every tree have same height
        - Perfect tree, too strong
    - Left and right subtree of each node's heights differ by at most 1
        - Ensures `root` height is $\Theta (log n)$
        - Quick: $\Theta (1)$ rotations
- AVL Balance Property: `balance(node) = height(node.left) - height(node.right)`
- AVL Tree: a BST with guaranteed balancing
    - BST with the added condition that for all `node`s, `-1 <= balance(node) <= 1`
    - We need to keep track of the heights

---

## Lecture 8 - June 7

- AVL Operations:
    - Find: same as BST
    - Insert: BST insert, then check and fix balance
- Let `p` be the problem node where an imbalance occurs
- Two main cases: inserted node is in the outside or inside (e.g. right-right vs right-left)
    - Outside cases are solved with a "single rotation"
    - Inside cases are solved with a "double rotation"
- Insert:
    - BST insert: afterwards, each node's height in the path to the bottom *might* have changed
    - Recursive backtracking: calculate new height and detect any height imbalances
    - If imbalance, find case and rotate. Only the deepest `p` needs to be addressed
- Single rotation: (left-left)
    - Move child of `p` to position of `p`
    - `p` becomes the "other" child
    - Old "other" child takes `p`'s new empty child
    - Other subtrees move accordingly
- Double rotation:
    - Rotate into an outside case with `p`'s child and grandchild
    - Rotate `p` and `p`'s new child
        - These rotations are in different directions

---

## Lecture 9 - June 10

- Motivation behind B-Trees: $\Theta (1)$ memory access operations are not insignificant
    - Getting data from disk is many, many operations (even if linear time)
    - Thus, minimizing disk access is a goal
    - This is of course if data doesn't fit in cache (i.e. databases)
- If we're accessing a byte in memory, might as well access a whole block and store it in cache
    - Temporal locality: if an address is referenced, it tends to be referenced
    - Spatial locality: if an address is referenced, addresses close by are more likely to be referenced soon
- The amount of data moved from disk to memory is block size, memory to cache is line size
    - These are not under program control
- Problem: large dictionaries requre that most of memory will be stored on disk
- Desire: a balanced tree that is even shallower than AVL trees to minimize disk accesses
- A key idea: increase branching factor of the tree
- B Trees:
    - Two types of nodes
        - Internal nodes, having M-1 keys and M children
        - Leaf nodes, storing up to L sorted data items
    - Order property:
        - Subtree between keys a and b contains only data which is: a <= x < b
    - Structure properties:
        - Root: If tree has <= L items, root is a leaf, else has between 2 and M children
        - Internal nodes: have between ceil(M/2) and M children (i.e. at least half full)
        - Leaf nodes: all leaves at the same depth, have between ceil(L/2) and L data items
    - Any M > 2 and L will work, but we pick them based on disk-block size
- Many trees stored in one internal node, binary search can be inconsequential compared to disk access
- Only bring one leaf of data items into memory, no need for loading unnecessary items into memory
- Adding to a leaf which is not already full is easy, just add like a sorted list
- When a leaf is full, we split it into two leaves and add them to the parent
- If the parent is full (so we cannot add another child), we split the parent
- If the parents parent ... we eventually split the root, where we create a new root with the two children
    - This is how a B-Tree gains height
- Deletion:
    - First remove the item
    - Then if the leaf contains too few, adopt from a neighbor
        - Unless neighbor is at minimum, then combine to create new child
        - If parent is now at minimum, adopt from a neighbor
        - repeat...

---

## Lecture 10 - June 12

- Now switching from tree-based data structures to array-based ones
- Hashing: take in a key, convert it into an integer
    - This allows us to index into an array with the integer
- If two keys have the same hash, we have a collision
    - Always will happen: infinite keys, finite array length
- Hash table runtime average is close to $\Theta (1)$
- Hash tables don't allow us to order well unlike trees
- We expect the number of key-value pairs to be much less than the possible domain
- Ideal hash function is fast and avoids excessive collisions
- Often the case that the client implements some hash function, then program mods by the array length
    - In java, the client will override the `hashCode` method
- Use prime number as a table size
    - Real life data tends to follow a pattern, primes don't follow them
- Try to use the most identifiable fields into the hash function
- Trade-off between how many fields and amount of collisions
    - We use *educated guesses*
- If the key space is an int, using int and modulo is intuitive
    - Even if every value has a distinct key, still collisions because mod
- Number one step is to rely on expertise of others
- Separate chaining:
    - All keys that map to the same table location are added to a list
- Worst case runtime for `find` is $\Theta (n)$
    - Not worth optimizing if your hash function isn't terrible
- Load factor is the average number of elements per bucket
    - Want to keep load factor constant, prevent it from being too large
    - Typically if lambda is greater than one, increase the table size
- `delete` is the same as `find` intuitively
