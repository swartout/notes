---
layout: default
title: CSE 493
parent: CSE
---

# CSE 493 - Deep Learning

[Course Website](https://courses.cs.washington.edu/courses/cse493g1/23sp/)
- This course is heavily based off of [CS231n](https://cs231n.github.io)
- This course will primarily focus on NLP and Computer Vision
- Course is multiple parts:
    - Deep learning fundamentals
    - Practical training skills
    - Application
- Vision has been one of the drivers of early DL, important to understand the history
- Books can be helpful, but not necessary
- Gradescope has automatic and private testing
- Three psets, one midterm, and a final group project

---

## Lecture 1 - March 28

- 543 million years ago vision animals started to develop sight
- Camera obscura developed in 1545 to study eclipses
    - Inspired da Vinci to create the pinhole camera
- 1959 Hubel & Wiesel found that we visually react to "edges" and "blobs"
    - Think of this a "lower layer"
- Larry Roberts is known as the "Father of Computer Vision" - wrote the first CV thesis
- 1960's MIT attempted to solve vision in a summer - this didn't happen
- David Marr introduced the idea of stages of visual recognition in 1970's
- Edge detection became the next big push in CV
- In the 1980's expert systems became popular
    - These had heuristic rules made by domain "experts"
    - Unsuccessful and caused the second AI Winter
- Irving Biederman came up with rules on how we view the world
    - 1: We must understand components (objects and relationships)
    - 2: This is only possible because we see so many objects learning
        - A 6 year old child has seen 30,000 objects
- We can detect animals in 150 ms
    - We detect predators and the color red even quicker!
- Later-stage neurons allowed us to detect complex object or themes
- In the 1990's research started on start on real-world images
    - Algorithms were developed for grouping (1990's) and matching (2000's)
- In 2001 the first commercial success in CV
    - Facial detection, used ML and facial features
- In the 2000's feature development was all the rage
    - Histogram of gradients - how do the edges in pixels move?
- We need an incredible amount of data - led to ImageNet
    - 2009, had 22K categories and 14M images
- In 2012 AlexNet had breakthrough performance on ImageNet
    - By 2015 all attempts were DL and better than humans
- In 1957 the Mark I Perception was created for character recognition
    - Manually twisted knobs to tune (adjusted weights)
    - Cannot be trained practically
- Backpropagation was developed in 1986
- LeNet is the architecture used in the Postal Service - 1998
    - AlexNet is the same architecture
- DL was used in the early 2000's to compress images
- Everything is homogenized now
    - Transformers and backprop are the norm
    - Data and compute are the differentiators
    - Domains change, but core is often the same
- Hinton, Bengio, and LeCun won the Turing award in 2018
- Deep learning is it's own course because of incredible growth

---

## Lecture 2 - March 30

- Image classification (IC) is a core task in CV
- There are many challenges related to computer vision:
    - Viewpoint variation
    - Illumination
    - Background clutter
    - Occlusion
    - Deformation
    - Intraclass variation
    - It is very difficult to implement an image classifier as "normal" software
        - "old" AI
    - Data-driven paradigm is better
        1. Use datasets to train a model
            - MNIST
            - CFIAR 10(0)
            - ImageNet
            - MIT Places
            - Omniglot
        2. Use ML to train a classifier
        3. Evaluate the classifier on new images
- Nearest Neighbor classifier
    - Training: memorize all data and labels
    - Inference: predict label of "nearest" image
    - $O(1)$ train time, $O(n)$ inference time
    - It is a universal approximator with n -> infinity data points
- What is "nearest" (distance)?
    - L1 norm is bad (Manhattan)
    - L2 norm is good (Euclidean)
 - Hyperparaamters are choices (configs) of the model
    - e.g. k, distance type
- Finding hyperparams
    - We should use a train, val, and test ds
    - Cross validation: split data into folds - no dedicated val necessary
- Curse of dimensionality: the number of data points necessary is related to the dimension of data
- Linear classifiers take on the form: $y = Wx + b$
    - $b$ is often omitted, instead appending data vector with a one
    - Parametric approach
    - Learns a template and decision boundary

---

## Lecture 3 - April 4

- It's cool that we have a classifier, but how do we make it good?
- Loss function: define how good or bad our classifier (weights) are
- Loss over dataset is the average of loss over examples
    - $x_i$, $y_i$, and $L_i$ are example data, output label, and example loss
- Multiclass SVM loss:
    - Make sure that prediction of correct label is greater than all predictions for other labels by a given margin
    - Delta doesn't matter because it will simply scale
    - Linear-ish loss
    - Issues because piecewise function, doesn't approach zero asymptotically
- Squaring losses leads to predictions being penalized by more by a factor of incorrect-ness
- Regularization is important, we always want the simplest model (no overfitting!)
    - Often adding a fraction of the L1 or L2 on the weights.
    - Spread out the weights
- Softmax classifier:
    - Pushes each value between 0 and 1
    - Ensures the sum of the softmaxed outputs is 1
    - $S_i = \frac{e^{y_i}}{\sum e^{y_i}}$
    - Scaling will determine how "peaky" the softmaxed scores are
- NLL is the negative log of the prediction of the correct label
- Cross entropy loss is the NLL of the softmax of the prediction of the correct
label
- Optimization is gradient descent
    - Find the partial derivatives of the loss with respect to the weights
    - We update each weight by the scaled negative of its partial derivative gradient 
- Use a numeric gradient to gradient check

---

## Lecture 4 - April 6

- Stochastic gradient descent uses minibatches to estimate the gradient
    - Pick your GPU, then biggest minibatch size typically
- Linear classifiers aren't very powerful (for many problems)
    - They only learn one template and linear decision boundaries
- We can extract features, *then* fit a linear classifier
    - Non-linear transformations (features) are necessary
    - We have to manually create features...if only we could learn them...
- Simple two-layer NN: $f = W_2 max(0, W_1x)$
    - $x \in \mathbb{R}^D, W_1 \in \mathbb{R}^{H \times D}, W_2 \in \mathbb{R}^{C \times H}$
    - We can think of this as learning templates, then learning combinations of templates
- There are many different activation functions:
    - ReLU, Sigmoid, Leaky ReLU are (historically) popular ones
- Activation functions are necessary because multiple consecutive linear transformations can be represented as just one transformation
- Biological neurons are *quite* different!
- We use computational graphs and backpropagation to differentiate (backwards pass)
- Backprop is simply the chain rule - a lot

---

## Lecture 5 - April 11

- Vector-vector backprop includes the Jacobian matrix for local operations
- ImageNet helped people realize that data was super important!
- Convolutional NNs are a way of including spatial information
- CNNs are ubiquitous within vision
- While FC nets flatten images, CNNs preserve spatial structure
- Filters must be the same depth as the input image (i.e. 3 for RGB)
- Slide over the entire image, flatten each part of the image it is above, dot product with filter
- Stride is the offset between each filter-comparison
- The number of filters is the number of activation weights (and the number of output channels)
- ConvNet is simply many convolutional layers with activation functions in between
- Earlier layers learn low-level features, later layers learn higher-level features
- Ends with a simple FC classifier
- There are interspersed pooling layers to downsample
- Output size of an $N \times N$ image with filter size $F \times F$ is: $(N - F) / stride + 1$
- Often images will be padded with zero pixels
- 1x1 convolutional layers increase or reduce depth in channel space

---

## Lecture 6 - April 13

- Training loop:
    - Sample a batch of data
    - Forward prop through the graph to get loss
    - Backprop through the gradients
    - Update the params using the gradient
- Before you train:
    - Activation functions
    - Preprocessing
    - Weight initialization
    - Regularization
    - Gradient checking
- Training dynamics:
    - Babysitting the learning process
    - Param updates
    - Hyperparam updates
- Evaluation:
    - Model ensamples
    - Test-time augmentation
    - Transfer learning
- Sigmoid function: $\sigma(x) = \frac{1}{1 + e^{-x}}$
    - Used to be popular
    - Saturated gradient at high positive and negative values
    - All gradients squashed by at least a factor of 4
    - Outputs aren't zero-centered which means local gradient is always positive
        - Weights will now all change in the same direction
    - Computationally expensive!
- Tanh function:
    - Zero centered
    - Still has the problem of dying gradients
- ReLU (rectified linear unit): $f(x) = max(0, x)$
    - Most common activation function
    - No saturation in positive region
    - Computationally efficient
    - Converges (6x) faster than sigmoid or tanh in practice
    - Not zero centered
    - Can lead to "dying" ReLUs
        - To prevent, often initialize biases with small positive value (e.g. 0.01)
- Leaky ReLU: $f(x) = max(0.01x, x)$
    - Same a ReLU, but minor gradient in negative region
    - Parametric Rectifier (PReLU) where scaling is a learnable parameter
- Exponential Linear Units (ELU):
    - "Better", but more expensive
- Scaled exponential linear units (SELU):
    - Works better for larger networks, has a normalizing property
    - Can use without BatchNorm
    - Holy Heck Math
    - **"Cool"**
- Maxout:
    - Max of multiple linear layers
    - Multiplies the number of parameters :(
- Swish:
    - RL-created activation function
- GeLU:
    - Add some randomness to ReLU, then take average to find this
        - "Data dependent dropout"
    - Main activation function used (esp. in transformers)
- Use ReLU, GeLU if transformers, and try ReLU derivatives
- We often zero-mean and normalize our data as preprocessing
- Sometimes preprocessing involves PCA and whitening
- ResNet subtracted mean across channels and normalized with standard deviation
- A constant weight initialization leads to all the values being the same
- Initializations that are too large or too small lead to extreme saturation or clustered outputs
- "Xavier" initialization: $std = \frac{1}{\sqrt{D_{in}}}$
    - Good with Tanh
    - Attempts to keep output variance similar to input variance
- "Kaiming" initialization: $std = \sqrt{\frac{2}{D_{in}}}$
    - Works well with ReLU!
- Batch Normalization:
    - "Things break when inputs don't have zero mean and $std = 1$" - "Why not just force that?"
    - Subtract by batch mean, divide by square root of the variance of the data
    - It is differentiable!
    - We keep these *before* each non-linearity
    - We also have two learned parameters, gamma and beta corresponding to scaling and shifting
    - We keep a running mean of mean and variance across our training process

---

## Lecture 7 - April 18

- BatchNorm
    - Becomes a linear layer during inference
    - "Resets" the standard deviation and mean after change from linear layers
    - Allows higher learning rate and better gradient flow
    - Acts as regularization during training
    - Different behavior during training and testing! Can be a common bug!
    - For CNNs, batch norm across channels (output is: $1 \times C \times 1 \times 1$)
- LayerNorm:
    - Normalize across each example!
- Instance normalization is for CNN across height and width
    - Good for segmentation and detection
- There are a ton of ways to normalize in similar ways
- Vanilla gradient descent: calculate gradient, move towards the negative gradient
    - Issues related to getting stuck in saddle points
    - Stochastic, so descent can be extremely noisy
- Momentum keeps optimization moving in the same direction:
    - $v_{t+1} = \rho v_t + \nabla f(x_t)$ then update using $v_{t+1}$
    - Common values for $\rho$ are $0.9$ and $0.99$
    - Momentum will often overshoot minima
- Nesterov momentum: take the velocity update *before* taking derivative
    - $$v_{t+1} &= \rho v_t - \alpha \nabla f(x_t + \rho v_t)$$
    - $$x_{t+1} &= x_t + v_{t+1}$$
    - Good, but we have to update weights twice, so we use a different formulation where $\tilde{x}_t = x_t + \rho v_t$:
        - $$v_{t+1} &= \rho v_t - \alpha \nabla f(\tilde{x}_t)$$
        - $$\tilde{x}_{t+1} &= \tilde{x}_t - \rho v_t + (1 + \rho) v_{t+1}$$
        - $$&= \tilde{x}_t + v_{t+1} + \rho (v_{t+1} - v_t)$$
- SGD + momentum or Nesterov are often what is used in practice
- AdaGrad sums squared squared gradients, dividing each gradient by the square root of the sum of its squared derivatives:
    - This leads to weights with large and small gradients getting updated slower and quicker
    - It also quickly decays all learning rates to zero - a problem for neural networks
- RMSProp (Leaky AdaGrad) is AdaGrad but with a decay to previous squared grads - similar to a running average
    - Keeps step sizes relatively constant
    - Common decay rate is $0.99$
    - Doesn't overshoot as much
    - More computationally expensive
- Adam: combine the best of RMSProp and AdaGrad
    ```python
    first_moment = 0
    second_moment = 0
    for t in range(1, num_iterations):
        dx = compute_gradient(x)
        first_moment = beta1 * first_moment + (1 - beta1) * dx
        second_moment = beta2 * second_moment + (1 - beta2) * dx * dx
        first_unbias = first_moment / (1 - beta1 ** t)
        second_unbias = second_moment / (1 - beta2 ** t)
        x -= learning rate * first_unbias / (np.sqrt(second_unbias) + 1e-7)
    ```
    - There is a bias correction to prevent large early step sizes from instantly destroying initialization
    - Typical hyperparams are `beta1 = 0.9` and `beta2 = 0.99`
- L2 regularization and weight decay are the same when using SGD (with momentum), but different for Adam, AdaGrad, RMSProp    
- AdamW: the go-to optimizer
    - Adam with decoupled weight decay and $L_2$ regularization
    - Allows user to choose if they want weights to be a part of the second moment or not
- There are second-order optimization techniques where you move in the negative direction of the Hessian
    - This is typically intractable due to the number of parameters
    - AdaGrad is actually a special case of a second-order optimization technique where we assume the Hessian is diagonal
    - Second-order optimization is not typically used in practice

---

## Lecture 8 - April 20

- Learning rate decay: scaling down the learning rate over time
    - Necessary to decrease loss beyond a certain point
    - Hyperparameter choice is extremely important
- There are many learning rate schedulers:
    - "Step" down the learning rate after a fixed number of epochs
        - Leads to massive drops in loss followed by plateaus repeatedly
    - Cosine learning rate decay: $\alpha_t = \frac{1}{2}\alpha_0(1+cos(\frac{t\pi}{T}))$
        - $\alpha_0$: initial learning rate, $\alpha_t$ learning rate at step $t$, $T$ total number of steps
        - Constant decrease in loss over time
    - Linear learning rate decay
    - Inverse square root decay
    - Constant learning rate decay
- Learning rate warmup: spending first iterations increasing learning rate
    - A large initial learning rate will cause our weight initialization to blow up
    - Linearly increase learning rate over about 5 epochs (~5000 iterations)
    - If you increase batch size by $N$, also increase learning rate by $N$
- Validation data is a good way of paying attention to the model
- Early stopping is ending model training when test loss plateaus
    - Often for ~5 epochs
- Training multiple models and then averaging results is a model ensemble:
    - Exhibits about ~2% better performance in the real world
    - Different models overfit to different parts of the dataset, it is averaged out
- Regularization techniques previously covered: L2, L1, weight decay
- Dropout: every forward pass, set parameters to zero with probability $p$
    - Increases redundancy across the entire network, prevents some overfitting
    - Common value of $p = 0.5$
    - An interpretation of dropout is an ensemble of models with shared parameters
    - At test time, multiply by $p$ **or** multiply by $\frac{1}{p}$ after each test example (inverted dropout)
- A common pattern with regularization is adding randomness during training and averaging out during testing
    - Seen in batch norm and dropout for example
- Data augmentation is a common form of regularization:
    - Transforming the input in such a way that the label is the same
- Some example image augmentation techniques:
    - Flipping an image horizontally
    - Random crops and scales of an image
        - Testing: average a fixed set of crops of a test image
    - Change contrast or color of images
    - Stretching or contorting images
- We are now training models to learn good data augmentation techniques
- DropConnect: set connections to weights to zero (instead of weights in dropout)
- Fractional pooling pools random regions of each image
    - Testing: average predictions over multiple regions
- Stochastic depth: skip entire layers in a network using residual connections
- Cutout: randomly set parts of image to average image color
    - Good for small datasets, not often used on large ones
- Mixup: blend both training images and training labels by an amount
    - Why does it work? Who cares!
- CutMix: replace random crops of one image with another while combining labels
- Label smoothing: set target label to: $1 - \frac{K-1}{K}\epsilon$ and other labels to: $\frac{\epsilon}{K}$ for $K$ classes
- In practice:
    - Use dropout for FC layers
    - Using batchnorm is always good
    - Try Cutout, MixUp, CutMix, Stochastic Depth, Label Smoothing for (a little) extra performance
- Grid search is okay for hyperparameter search
    - Use log-linear values
- Random search is better:
    - Use log-uniform randomness in given range
    - Likely because some hyperparamters matter more than others, so more opportunities to get it exact than grid search
- How to choose hyperparameters without Google-level compute:
    1. Check initial loss: sanity check, turn off weight decay
    2. Overfit on a small sample (5-10 minibatches)
        - Make some architectural decisions
        - Loss not decreasing? LR too low, bad weight init
        - Loss NaN or Inf? LR too high, also bad weight init
    3. Search for learning rate for ~100 iterations
        - Good LR to think about: 1e-1 to 1e-4
    4. Coarse search: add other hyperparams and train a few models for ~1.5 epochs
        - Good weight decay to try: 1e-4, 1e-5, 0
    5. Pick best models from 4., train for ~10-20 epochs without learning rate decay
    6. Look at learning curves and adjust
        - Might need early stopping, adjust regularization, larger model, or **keep going**
        - Flat start to loss graph means bad initialization
        - When learning rate plateaus, add a scheduler (cosine!) 
        - Use a "command center" like Weights and Biases (or add your own!)
    7. Return to step 5!
- Linear classifiers are easy to visualize because they have only one filter
- Early layers in deep learning networks are also easy to visualize, these are filters
    - Often edge detection
- The last layers are an embedded representation of the inputs
    - The embeddings are much better for KNN
- Google search:
    - Embeds search into a 100 or 200 dimensional vector, then runs KNN on a massive database
    - Needs massive compute however
- We can use PCE or t-SNE to visualize the outputs of the last layer of a network
- Good to use KNN rather than simply labels so we can see what is happening under the hood
- We can visualize activation maps for CNNs
- To find what activates neurons the best, run patches of images from the dataset through the model and see which output the full image's label
- Occlusion using patches is another way of visualizing which pixels matter, can be graphed for a cool image
- Shapley values are a way of using multiple patches to determine important areas of the image
- Compute backprop to the images to find activations for saliency maps
    - Super good image segmentation (accidentally)

---

## Lecture 8 - April 25

- Saliency maps allow us to view biases within misclassifications
    - Clamp gradients to only negative values
- We can also "backprop" a gray image to make an example image
    - Gradient ascent and a heck of a lot of regularization
- Adversarial examples:
    1. Start from an arbitrary image
    2. Pick an arbitrary class
    3. Modify the image to improve class scores
    4. Repeat!
- Black box attacks:
    - Adding random noise to images and models get confused...
- Supervised learning is insanely expensive:
    - Labeling ImageNet's 1.4M images would cost more than $175,000
- Unsupervised learning: model isn't told what to predict
- Self-Supervised learning: model predicts some naturally occurring signal in the data
    - The goal is to learn a cheap "pretext" task which learns important features
    - Target is something that is easy to compute
- For example, start with autoencoder, then fine-tune
- Three main types of pretext tasks:
    - Generative
        - Autoencoders, GANs
    - Discriminative
        - Contrastive
    - Multimodal
        - Input video, output audio
- Pretext task performance is irrelevant
- Often just toss a linear classifier on the back of the encoder
- Generative supervised learning:
    - Generate some data from an example
- Computers (NNs) are not rotation-invariant:
    - Allows us to predict rotation as a pretext task
- The learned attentions are similar as to what supervised learning finds
- Predict relative patch locations:
    - Break image into 3x3 grid, pass in center square and an outer square and predict location
    - CNNs are size invariant
- Jigsaw puzzle:
    - Reorder patches according to correct permutation
- Inpainting:
    - Pass in image with missing patch, predict the missing patch
    - Adding adversarial loss increases image recreation quality
- Image coloring:
    - Use "LAB" coloring, pass in L (grayscale) and predict AB (color)
- Split-brain autoencoder:
    - Predict color from light, predict light from color
- Video coloring:
    - Given colored start frame and grayscale images from then on, predict the colors
    - Uses attention mechanism
- Contrastive learning:
    - Create many different examples from an orignal example
    - These examples are going to be in the same sematic space
    - Learn which examples are closer or further away from each other
