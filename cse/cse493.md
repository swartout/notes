---
layout: default
title: CSE 493
parent: CSE
---

# CSE 493 - Deep Learning

[Course Website](https://courses.cs.washington.edu/courses/cse493g1/23sp/)
- This course is heavily based off of [CS231n](https://cs231n.github.io)
- This course will primarily focus on NLP and Computer Vision
- Course is multiple parts:
    - Deep learning fundamentals
    - Practical training skills
    - Application
- Vision has been one of the drivers of early DL, important to understand the history
- Books can be helpful, but not necessary
- Gradescope has automatic and private testing
- Three psets, one midterm, and a final group project

---

## Lecture 1 - March 28

- 543 million years ago vision animals started to develop sight
- Camera obscura developed in 1545 to study eclipses
    - Inspired da Vinci to create the pinhole camera
- 1959 Hubel & Wiesel found that we visually react to "edges" and "blobs"
    - Think of this a "lower layer"
- Larry Roberts is known as the "Father of Computer Vision" - wrote the first CV thesis
- 1960's MIT attempted to solve vision in a summer - this didn't happen
- David Marr introduced the idea of stages of visual recognition in 1970's
- Edge detection became the next big push in CV
- In the 1980's expert systems became popular
    - These had heuristic rules made by domain "experts"
    - Unsuccessful and caused the second AI Winter
- Irving Biederman came up with rules on how we view the world
    - 1: We must understand components (objects and relationships)
    - 2: This is only possible because we see so many objects learning
        - A 6 year old child has seen 30,000 objects
- We can detect animals in 150 ms
    - We detect predators and the color red even quicker!
- Later-stage neurons allowed us to detect complex object or themes
- In the 1990's research started on start on real-world images
    - Algorithms were developed for grouping (1990's) and matching (2000's)
- In 2001 the first commercial success in CV
    - Facial detection, used ML and facial features
- In the 2000's feature development was all the rage
    - Histogram of gradients - how do the edges in pixels move?
- We need an incredible amount of data - led to ImageNet
    - 2009, had 22K categories and 14M images
- In 2012 AlexNet had breakthrough performance on ImageNet
    - By 2015 all attempts were DL and better than humans
- In 1957 the Mark I Perception was created for character recognition
    - Manually twisted knobs to tune (adjusted weights)
    - Cannot be trained practically
- Backpropagation was developed in 1986
- LeNet is the architecture used in the Postal Service - 1998
    - AlexNet is the same architecture
- DL was used in the early 2000's to compress images
- Everything is homogenized now
    - Transformers and backprop are the norm
    - Data and compute are the differentiators
    - Domains change, but core is often the same
- Hinton, Bengio, and LeCun won the Turing award in 2018
- Deep learning is it's own course because of incredible growth

---

## Lecture 2 - March 30

- Image classification (IC) is a core task in CV
- There are many challenges related to computer vision:
    - Viewpoint variation
    - Illumination
    - Background clutter
    - Occlusion
    - Deformation
    - Intraclass variation
    - It is very difficult to implement an image classifier as "normal" software
        - "old" AI
    - Data-driven paradigm is better
        1. Use datasets to train a model
            - MNIST
            - CFIAR 10(0)
            - ImageNet
            - MIT Places
            - Omniglot
        2. Use ML to train a classifier
        3. Evaluate the classifier on new images
- Nearest Neighbor classifier
    - Training: memorize all data and labels
    - Inference: predict label of "nearest" image
    - $O(1)$ train time, $O(n)$ inference time
    - It is a universal approximator with n -> infinity data points
- What is "nearest" (distance)?
    - L1 norm is bad (Manhattan)
    - L2 norm is good (Euclidean)
 - Hyperparaamters are choices (configs) of the model
    - e.g. k, distance type
- Finding hyperparams
    - We should use a train, val, and test ds
    - Cross validation: split data into folds - no dedicated val necessary
- Curse of dimensionality: the number of data points necessary is related to the dimension of data
- Linear classifiers take on the form: $y = Wx + b$
    - $b$ is often omitted, instead appending data vector with a one
    - Parametric approach
    - Learns a template and decision boundary

---

## Lecture 3 - April 4

- It's cool that we have a classifier, but how do we make it good?
- Loss function: define how good or bad our classifier (weights) are
- Loss over dataset is the average of loss over examples
    - $x_i$, $y_i$, and $L_i$ are example data, output label, and example loss
- Multiclass SVM loss:
    - Make sure that prediction of correct label is greater than all predictions for other labels by a given margin
    - Delta doesn't matter because it will simply scale
    - Linear-ish loss
    - Issues because piecewise function, doesn't approach zero asymptotically
- Squaring losses leads to predictions being penalized by more by a factor of incorrect-ness
- Regularization is important, we always want the simplest model (no overfitting!)
    - Often adding a fraction of the L1 or L2 on the weights.
    - Spread out the weights
- Softmax classifier:
    - Pushes each value between 0 and 1
    - Ensures the sum of the softmaxed outputs is 1
    - $S_i = \frac{e^{y_i}}{\sum e^{y_i}}$
    - Scaling will determine how "peaky" the softmaxed scores are
- NLL is the negative log of the prediction of the correct label
- Cross entropy loss is the NLL of the softmax of the prediction of the correct
label
- Optimization is gradient descent
    - Find the partial derivatives of the loss with respect to the weights
    - We update each weight by the scaled negative of its partial derivative gradient 
- Use a numeric gradient to gradient check