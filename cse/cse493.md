---
layout: default
title: CSE 493
parent: CSE
---

# CSE 493 - Deep Learning

[Course Website](https://courses.cs.washington.edu/courses/cse493g1/23sp/)
- This course is heavily based off of [CS231n](https://cs231n.github.io)
- This course will primarily focus on NLP and Computer Vision
- Course is multiple parts:
    - Deep learning fundamentals
    - Practical training skills
    - Application
- Vision has been one of the drivers of early DL, important to understand the history
- Books can be helpful, but not necessary
- Gradescope has automatic and private testing
- Three psets, one midterm, and a final group project
---

## Lecture 1 - March 28

- 543 million years ago vision animals started to develop sight
- Camera obscura developed in 1545 to study eclipses
    - Inspired da Vinci to create the pinhole camera
- 1959 Hubel & Wiesel found that we visually react to "edges" and "blobs"
    - Think of this a "lower layer"
- Larry Roberts is known as the "Father of Computer Vision" - wrote the first CV thesis
- 1960's MIT attempted to solve vision in a summer - this didn't happen
- David Marr introduced the idea of stages of visual recognition in 1970's
- Edge detection became the next big push in CV
- In the 1980's expert systems became popular
    - These had heuristic rules made by domain "experts"
    - Unsuccessful and caused the second AI Winter
- Irving Biederman came up with rules on how we view the world
    - 1: We must understand components (objects and relationships)
    - 2: This is only possible because we see so many objects learning
        - A 6 year old child has seen 30,000 objects
- We can detect animals in 150 ms
    - We detect predators and the color red even quicker!
- Later-stage neurons allowed us to detect complex object or themes
- In the 1990's research started on start on real-world images
    - Algorithms were developed for grouping (1990's) and matching (2000's)
- In 2001 the first commercial success in CV
    - Facial detection, used ML and facial features
- In the 2000's feature development was all the rage
    - Histogram of gradients - how do the edges in pixels move?
- We need an incredible amount of data - led to ImageNet
    - 2009, had 22K categories and 14M images
- In 2012 AlexNet had breakthrough performance on ImageNet
    - By 2015 all attempts were DL and better than humans
- In 1957 the Mark I Perception was created for character recognition
    - Manually twisted knobs to tune (adjusted weights)
    - Cannot be trained practically
- Backpropagation was developed in 1986
- LeNet is the architecture used in the Postal Service - 1998
    - AlexNet is the same architecture
- DL was used in the early 2000's to compress images
- Everything is homogenized now
    - Transformers and backprop are the norm
    - Data and compute are the differentiators
    - Domains change, but core is often the same
- Hinton, Bengio, and LeCun won the Turing award in 2018
- Deep learning is it's own course because of incredible growth